First of all. Excuse me for my english writing-skills.   Being a Norwegian citizen with free universal healthcare, it is pretty hard to understand why many of you Americans are so afraid of a little more govermental influence in the healtcare-business. Everybody knows that healthcare costs money. So why accept that a large ammount of your money is going right in the pockets of the insurancecompanies? Shouldn´t your money result in healthcare and not in profit for some greedy company?   To be honest I´m happy to be Norwegian instead of American.   When I get sick I can go to any hospital and get the best treatment available without having to worry about the costs. What´s wrong with that guys? Of course it comes at a price. We probably pay higher taxes than you. But I do not pay for insurance, and bottom line I think most people are better off this way. At least I´m getting healthcare for my money without supporting insurancecompanies that makes a profit out of peoples health-issues.   Any comments?